{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression : 2진 분류(Binary Classification)\n",
    ": 2진 분류의 활성화 함수로는 sigmoid가 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 함수\n",
    "#### sigmoid : 2진 분류(Binary Classification,Logistic Regression) 모델의  활성화 함수(Activation Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "3.7200759760208555e-44\n",
      "4.539786870243442e-05\n",
      "0.5\n",
      "0.9999546021312976\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + math.e**-z)\n",
    "\n",
    "print(math.e) # 2.718281828459045\n",
    "\n",
    "print(sigmoid(-100)) # 0에 수렴\n",
    "print(sigmoid(-10))\n",
    "print(sigmoid(0))    # 0.5\n",
    "print(sigmoid(10))\n",
    "print(sigmoid(100))  # 1에 수렴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "x,y = [],[]\n",
    "for k in range(-100,101):\n",
    "    n = sigmoid(k/10)\n",
    "    \n",
    "    x.append(k/10)\n",
    "    y.append(n)\n",
    "    \n",
    "plt.plot(x,y,'ro')  \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression : 2진 분류(Binary Classification)¶\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data : [6,2]\n",
    "x_data = [[1,2],\n",
    "          [2,3],\n",
    "          [3,1],\n",
    "          [4,3],\n",
    "          [5,3],\n",
    "          [6,2]]\n",
    "\n",
    "# y_data : [6,1]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "x_train = np.array(x_data,dtype=np.float32)\n",
    "y_train = np.array(y_data,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 초기화 : weight, bias\n",
    "# (m,n) * (n,l) = (m,l)   : 행렬의 내적 곱셈 공식\n",
    "# (6,2) * (2,1) = (6,1)\n",
    "W = tf.Variable(tf.random.normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수(hypothesis) : H(X) = sigmoid(X*W + b)\n",
    "# tf.sigmoid() : tf.div(1.,1. + tf.exp(-(tf.matmul(X,W) + b)))\n",
    "def hypothesis(X):\n",
    "    return tf.sigmoid(tf.matmul(X,W) + b)   # 0과 1사이의 값이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용함수 : logloss, 2진 분류 모델\n",
    "def cost_func():\n",
    "    # cost = tf.reduce_mean(tf.square(hypothesis(x_train) - y_train)) # 회귀 모델\n",
    "\n",
    "    cost = -tf.reduce_mean(y_train*tf.math.log(hypothesis(x_train)) + \n",
    "                          (1 - y_train)*tf.math.log(1 - hypothesis(x_train)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법\n",
    "# learning_rate(학습율)을 0.01 로 설정하여 optimizer객체를 생성\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Start Learning!!\n",
      "0000 cost:[ 1.661137 ]  W: [[-0.17030667]\n",
      " [-0.9402863 ]]  b: [0.23652855]\n",
      "0100 cost:[ 0.588478 ]  W: [[ 0.36285377]\n",
      " [-0.5124408 ]]  b: [0.4072708]\n",
      "0200 cost:[ 0.5059313 ]  W: [[ 0.53627616]\n",
      " [-0.5368375 ]]  b: [-0.18216892]\n",
      "0300 cost:[ 0.4375107 ]  W: [[ 0.69217587]\n",
      " [-0.5114027 ]]  b: [-0.8036816]\n",
      "0400 cost:[ 0.38093838 ]  W: [[ 0.81929266]\n",
      " [-0.44354403]]  b: [-1.4264516]\n",
      "0500 cost:[ 0.332903 ]  W: [[ 0.9234858 ]\n",
      " [-0.34686092]]  b: [-2.0376947]\n",
      "0600 cost:[ 0.29171243 ]  W: [[ 1.0127202 ]\n",
      " [-0.23518287]]  b: [-2.6305726]\n",
      "0700 cost:[ 0.2564744 ]  W: [[ 1.0934263 ]\n",
      " [-0.11935537]]  b: [-3.2008498]\n",
      "0800 cost:[ 0.22647582 ]  W: [[ 1.1698232 ]\n",
      " [-0.00650668]]  b: [-3.7462757]\n",
      "0900 cost:[ 0.20100074 ]  W: [[1.2442577 ]\n",
      " [0.09951956]]  b: [-4.266259]\n",
      "1000 cost:[ 0.17934632 ]  W: [[1.3178085 ]\n",
      " [0.19723554]]  b: [-4.7614303]\n",
      "1100 cost:[ 0.16087212 ]  W: [[1.3908268]\n",
      " [0.2865707]]  b: [-5.2331715]\n",
      "1200 cost:[ 0.14502646 ]  W: [[1.4633183]\n",
      " [0.3681636]]  b: [-5.6832585]\n",
      "1300 cost:[ 0.13135158 ]  W: [[1.5351601 ]\n",
      " [0.44291767]]  b: [-6.1135893]\n",
      "1400 cost:[ 0.11947423 ]  W: [[1.6062117]\n",
      " [0.5117626]]  b: [-6.5260234]\n",
      "1500 cost:[ 0.10909265 ]  W: [[1.6763602]\n",
      " [0.5755497]]  b: [-6.9223123]\n",
      "1600 cost:[ 0.09996322 ]  W: [[1.7455318 ]\n",
      " [0.63501495]]  b: [-7.3040485]\n",
      "1700 cost:[ 0.091888696 ]  W: [[1.8136902]\n",
      " [0.6907785]]  b: [-7.672673]\n",
      "1800 cost:[ 0.0847089 ]  W: [[1.8808268]\n",
      " [0.7433554]]  b: [-8.029464]\n",
      "1900 cost:[ 0.078292996 ]  W: [[1.9469554 ]\n",
      " [0.79317254]]  b: [-8.375554]\n",
      "2000 cost:[ 0.07253334 ]  W: [[2.0121026]\n",
      " [0.8405846]]  b: [-8.711953]\n",
      "2100 cost:[ 0.06734073 ]  W: [[2.0763073]\n",
      " [0.885889 ]]  b: [-9.039557]\n",
      "2200 cost:[ 0.0626412 ]  W: [[2.139611 ]\n",
      " [0.9293341]]  b: [-9.359154]\n",
      "2300 cost:[ 0.058372572 ]  W: [[2.2020583]\n",
      " [0.9711315]]  b: [-9.67145]\n",
      "2400 cost:[ 0.054482583 ]  W: [[2.263695 ]\n",
      " [1.0114591]]  b: [-9.977069]\n",
      "2500 cost:[ 0.05092673 ]  W: [[2.3245664]\n",
      " [1.0504726]]  b: [-10.276576]\n",
      "2600 cost:[ 0.047667187 ]  W: [[2.384716 ]\n",
      " [1.0883049]]  b: [-10.570473]\n",
      "2700 cost:[ 0.04467153 ]  W: [[2.4441867]\n",
      " [1.1250709]]  b: [-10.859209]\n",
      "2800 cost:[ 0.041911718 ]  W: [[2.503018 ]\n",
      " [1.1608733]]  b: [-11.143195]\n",
      "2900 cost:[ 0.039363593 ]  W: [[2.5612485]\n",
      " [1.1958005]]  b: [-11.422799]\n",
      "3000 cost:[ 0.037006143 ]  W: [[2.6189137]\n",
      " [1.2299305]]  b: [-11.698348]\n",
      "3100 cost:[ 0.03482083 ]  W: [[2.676047 ]\n",
      " [1.2633332]]  b: [-11.970146]\n",
      "3200 cost:[ 0.032791637 ]  W: [[2.7326806]\n",
      " [1.296071 ]]  b: [-12.238466]\n",
      "3300 cost:[ 0.030904254 ]  W: [[2.7888427]\n",
      " [1.3281995]]  b: [-12.50356]\n",
      "3400 cost:[ 0.029146155 ]  W: [[2.8445628]\n",
      " [1.3597684]]  b: [-12.765656]\n",
      "3500 cost:[ 0.02750607 ]  W: [[2.8998656]\n",
      " [1.3908219]]  b: [-13.024961]\n",
      "3600 cost:[ 0.025974153 ]  W: [[2.954776]\n",
      " [1.421401]]  b: [-13.281669]\n",
      "3700 cost:[ 0.024541484 ]  W: [[3.0093162]\n",
      " [1.451542 ]]  b: [-13.5359535]\n",
      "3800 cost:[ 0.023200097 ]  W: [[3.0635076]\n",
      " [1.4812787]]  b: [-13.787976]\n",
      "3900 cost:[ 0.021942802 ]  W: [[3.1173713]\n",
      " [1.5106406]]  b: [-14.03789]\n",
      "4000 cost:[ 0.020763224 ]  W: [[3.170925 ]\n",
      " [1.5396553]]  b: [-14.28583]\n",
      "4100 cost:[ 0.019655505 ]  W: [[3.2241862]\n",
      " [1.5683484]]  b: [-14.531922]\n",
      "4200 cost:[ 0.018614303 ]  W: [[3.2771714]\n",
      " [1.5967425]]  b: [-14.776286]\n",
      "4300 cost:[ 0.017634872 ]  W: [[3.3298967]\n",
      " [1.6248593]]  b: [-15.019032]\n",
      "4400 cost:[ 0.016712874 ]  W: [[3.3823755]\n",
      " [1.652717 ]]  b: [-15.260257]\n",
      "4500 cost:[ 0.015844258 ]  W: [[3.4346223]\n",
      " [1.6803341]]  b: [-15.500059]\n",
      "4600 cost:[ 0.015025399 ]  W: [[3.4866498]\n",
      " [1.7077266]]  b: [-15.738523]\n",
      "4700 cost:[ 0.014253012 ]  W: [[3.5384705]\n",
      " [1.7349098]]  b: [-15.975731]\n",
      "4800 cost:[ 0.013523981 ]  W: [[3.5900955]\n",
      " [1.7618972]]  b: [-16.211763]\n",
      "4900 cost:[ 0.012835502 ]  W: [[3.6415348]\n",
      " [1.7887018]]  b: [-16.446676]\n",
      "5000 cost:[ 0.012184973 ]  W: [[3.6927993]\n",
      " [1.8153362]]  b: [-16.680557]\n",
      "5100 cost:[ 0.0115699945 ]  W: [[3.7438977]\n",
      " [1.8418113]]  b: [-16.913452]\n",
      "5200 cost:[ 0.010988378 ]  W: [[3.7948394]\n",
      " [1.8681358]]  b: [-17.145416]\n",
      "5300 cost:[ 0.010438103 ]  W: [[3.8456333]\n",
      " [1.8943208]]  b: [-17.376518]\n",
      "5400 cost:[ 0.009917222 ]  W: [[3.896286 ]\n",
      " [1.9203742]]  b: [-17.606794]\n",
      "5500 cost:[ 0.009423948 ]  W: [[3.9468064]\n",
      " [1.9463056]]  b: [-17.8363]\n",
      "5600 cost:[ 0.008956709 ]  W: [[3.9972003]\n",
      " [1.9721205]]  b: [-18.065079]\n",
      "5700 cost:[ 0.008513973 ]  W: [[4.0474734]\n",
      " [1.9978298]]  b: [-18.293165]\n",
      "5800 cost:[ 0.008094298 ]  W: [[4.0976357]\n",
      " [2.023436 ]]  b: [-18.520607]\n",
      "5900 cost:[ 0.0076963957 ]  W: [[4.1476927]\n",
      " [2.0489461]]  b: [-18.747438]\n",
      "6000 cost:[ 0.0073189805 ]  W: [[4.1976485]\n",
      " [2.0743663]]  b: [-18.97369]\n",
      "6100 cost:[ 0.0069609196 ]  W: [[4.2475066]\n",
      " [2.0997033]]  b: [-19.199394]\n",
      "6200 cost:[ 0.006621176 ]  W: [[4.2972727]\n",
      " [2.1249583]]  b: [-19.424587]\n",
      "6300 cost:[ 0.0062987185 ]  W: [[4.3469534]\n",
      " [2.1501398]]  b: [-19.64929]\n",
      "6400 cost:[ 0.0059925206 ]  W: [[4.396554 ]\n",
      " [2.1752524]]  b: [-19.87354]\n",
      "6500 cost:[ 0.0057018287 ]  W: [[4.446077 ]\n",
      " [2.2002993]]  b: [-20.097345]\n",
      "6600 cost:[ 0.0054256953 ]  W: [[4.4955277]\n",
      " [2.2252796]]  b: [-20.320744]\n",
      "6700 cost:[ 0.005163413 ]  W: [[4.544906 ]\n",
      " [2.2502058]]  b: [-20.543755]\n",
      "6800 cost:[ 0.004914188 ]  W: [[4.5942264]\n",
      " [2.2750695]]  b: [-20.766403]\n",
      "6900 cost:[ 0.0046773492 ]  W: [[4.6434736]\n",
      " [2.2998846]]  b: [-20.9887]\n",
      "7000 cost:[ 0.00445227 ]  W: [[4.692671 ]\n",
      " [2.3246496]]  b: [-21.210663]\n",
      "7100 cost:[ 0.0042382907 ]  W: [[4.7418056]\n",
      " [2.3493755]]  b: [-21.432327]\n",
      "7200 cost:[ 0.0040348694 ]  W: [[4.7908916]\n",
      " [2.3740485]]  b: [-21.653687]\n",
      "7300 cost:[ 0.0038414707 ]  W: [[4.8399253]\n",
      " [2.3986866]]  b: [-21.874786]\n",
      "7400 cost:[ 0.0036575543 ]  W: [[4.8889112]\n",
      " [2.4232817]]  b: [-22.095594]\n",
      "7500 cost:[ 0.0034826184 ]  W: [[4.9378533]\n",
      " [2.4478357]]  b: [-22.316172]\n",
      "7600 cost:[ 0.0033162152 ]  W: [[4.9867477]\n",
      " [2.4723656]]  b: [-22.536522]\n",
      "7700 cost:[ 0.003157935 ]  W: [[5.035604 ]\n",
      " [2.4968576]]  b: [-22.756638]\n",
      "7800 cost:[ 0.003007331 ]  W: [[5.084426]\n",
      " [2.521317]]  b: [-22.976542]\n",
      "7900 cost:[ 0.002864076 ]  W: [[5.1332064]\n",
      " [2.545754 ]]  b: [-23.196241]\n",
      "8000 cost:[ 0.0027277253 ]  W: [[5.1819453]\n",
      " [2.570162 ]]  b: [-23.415749]\n",
      "8100 cost:[ 0.002597983 ]  W: [[5.2306595]\n",
      " [2.5945344]]  b: [-23.635088]\n",
      "8200 cost:[ 0.0024745043 ]  W: [[5.2793446]\n",
      " [2.6188843]]  b: [-23.854242]\n",
      "8300 cost:[ 0.0023569854 ]  W: [[5.327991 ]\n",
      " [2.6432142]]  b: [-24.073233]\n",
      "8400 cost:[ 0.0022451028 ]  W: [[5.376609 ]\n",
      " [2.6675236]]  b: [-24.292067]\n",
      "8500 cost:[ 0.002138613 ]  W: [[5.425205 ]\n",
      " [2.6918154]]  b: [-24.510761]\n",
      "8600 cost:[ 0.0020372337 ]  W: [[5.4737797]\n",
      " [2.716076 ]]  b: [-24.729343]\n",
      "8700 cost:[ 0.001940742 ]  W: [[5.522323 ]\n",
      " [2.7403193]]  b: [-24.94774]\n",
      "8800 cost:[ 0.001848856 ]  W: [[5.570845 ]\n",
      " [2.7645512]]  b: [-25.16605]\n",
      "8900 cost:[ 0.0017613337 ]  W: [[5.6193395]\n",
      " [2.7887714]]  b: [-25.38425]\n",
      "9000 cost:[ 0.0016780436 ]  W: [[5.667814]\n",
      " [2.812966]]  b: [-25.602295]\n",
      "9100 cost:[ 0.0015986942 ]  W: [[5.7162733]\n",
      " [2.8371577]]  b: [-25.820286]\n",
      "9200 cost:[ 0.0015231641 ]  W: [[5.7647104]\n",
      " [2.8613074]]  b: [-26.038105]\n",
      "9300 cost:[ 0.0014511925 ]  W: [[5.8131413]\n",
      " [2.8854704]]  b: [-26.25592]\n",
      "9400 cost:[ 0.0013826685 ]  W: [[5.861541 ]\n",
      " [2.9095993]]  b: [-26.473549]\n",
      "9500 cost:[ 0.0013174111 ]  W: [[5.90994  ]\n",
      " [2.9337323]]  b: [-26.691177]\n",
      "9600 cost:[ 0.0012552496 ]  W: [[5.958306 ]\n",
      " [2.9578457]]  b: [-26.908663]\n",
      "9700 cost:[ 0.0011960338 ]  W: [[6.006657 ]\n",
      " [2.9819632]]  b: [-27.1261]\n",
      "9800 cost:[ 0.0011396528 ]  W: [[6.0550084]\n",
      " [3.0060692]]  b: [-27.343475]\n",
      "9900 cost:[ 0.0010859169 ]  W: [[6.10332  ]\n",
      " [3.0301511]]  b: [-27.560722]\n",
      "10000 cost:[ 0.001034755 ]  W: [[6.1516347]\n",
      " [3.0542414]]  b: [-27.77797]\n",
      "****** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "print('****** Start Learning!!')\n",
    "for step in range(10001):\n",
    "    # cost를 minimize 한다\n",
    "    optimizer.minimize(cost_func,var_list=[W,b]) # W,b를 업데이트\n",
    "    if step % 100 == 0:\n",
    "        print('%04d'%step,'cost:[',cost_func().numpy(),']',\n",
    "             ' W:',W.numpy(),' b:',b.numpy())\n",
    "        \n",
    "print('****** Learning Finished!!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [[6.1516347]\n",
      " [3.0542414]]\n",
      "bias: [-27.77797]\n"
     ]
    }
   ],
   "source": [
    "# Weight과 bias 출력\n",
    "print('Weight:',W.numpy())\n",
    "print('bias:', b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Hypothesis:\n",
      " [[1.8225627e-07]\n",
      " [1.8111591e-03]\n",
      " [1.8908737e-03]\n",
      " [9.9750584e-01]\n",
      " [9.9999464e-01]\n",
      " [9.9999976e-01]]\n",
      "Predict:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# 정확도 측정 : accuracy computation\n",
    "def predict(X):\n",
    "    return tf.cast(hypothesis(X) > 0.5, dtype=tf.float32)\n",
    "\n",
    "# 학습 데이터를 그대로 검증 데이터로 사용하여 예측한 경우\n",
    "x_test = x_train\n",
    "y_test = y_train\n",
    "\n",
    "preds = predict(x_test)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y_test),dtype=tf.float32))\n",
    "\n",
    "print('Accuracy:',accuracy.numpy())\n",
    "print('Hypothesis:\\n',hypothesis(x_test).numpy())\n",
    "print('Predict:\\n',preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Predict\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "print('***** Predict')\n",
    "# x_data = [[1,2],\n",
    "#           [2,3],\n",
    "#           [3,1],\n",
    "#           [4,3],\n",
    "#           [5,3],\n",
    "#           [6,2]]\n",
    "x_data = [[1,1],\n",
    "          [2,5],\n",
    "          [3,3],\n",
    "          [4,4],\n",
    "          [5,2],\n",
    "          [6,6]]\n",
    "x_test = np.array(x_data,dtype=np.float32)\n",
    "preds = predict(x_test)\n",
    "print(preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
